{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:44:01.600686Z",
     "start_time": "2020-04-04T01:44:01.496671Z"
    }
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:44:02.585168Z",
     "start_time": "2020-04-04T01:44:02.576192Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    }
   ],
   "source": [
    "# show all environments\n",
    "total_env = envs.registry.all()\n",
    "env_ids = [i.id for i in total_env]\n",
    "print(len(env_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:44:03.738080Z",
     "start_time": "2020-04-04T01:44:03.426853Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "#show the FrozenLake-v0 environment\n",
    "try:\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "except:\n",
    "    print(\"unable to load env!\")\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:16:00.122794Z",
     "start_time": "2020-04-04T01:16:00.117768Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4) <class 'gym.spaces.discrete.Discrete'>\n",
      "Discrete(16) <class 'gym.spaces.discrete.Discrete'>\n",
      "1\n",
      "True\n",
      "4\n",
      "4 16\n"
     ]
    }
   ],
   "source": [
    "# several key points of the environment\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "print(action_space, type(action_space))\n",
    "print(observation_space, type(observation_space))\n",
    "print(action_space.sample()) # randomly sample from 0,1,2,3\n",
    "print(observation_space.contains(14)) # judge wheather the state is in the state set\n",
    "print(action_space.n) # number of actions\n",
    "print(env.nA, env.nS) # number of actions in this env, number of states in this env.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T01:16:00.137714Z",
     "start_time": "2020-04-04T01:16:00.123750Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 14\n",
      "0 : [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "1 : [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "2 : [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "3 : [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "\n",
      "\n",
      "\n",
      "state 4\n",
      "0 : [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "1 : [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "2 : [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "3 : [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n",
      "state 5\n",
      "0 : [(1.0, 5, 0, True)]\n",
      "1 : [(1.0, 5, 0, True)]\n",
      "2 : [(1.0, 5, 0, True)]\n",
      "3 : [(1.0, 5, 0, True)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "check the 14th state's dynamic\n",
    "each state has four actions: 0=>up; 1=>down; 2=>left; 3=>right\n",
    "\n",
    "Notice that dynamic is different from policy.\n",
    "Policy determins the probability of choosing a action at a specific state.\n",
    "Dynamic relates to the environment, for example, imagine there's a boat driving on \n",
    "the lake, at a specific state it turned left, however, the place it finally reached\n",
    "is determined by the whole environment such as wind, stream, etc.\n",
    "\n",
    "When arrived at state[15], get 1 reward, then the episode is finished.\n",
    "When agent stuck into a \"Hole\" the episode finishs without any reward!\n",
    "'''\n",
    "print(\"state 14\")\n",
    "for key, value in env.P[14].items():\n",
    "    print(key,':', value)\n",
    "print('\\n'*2)\n",
    "print('state 4')\n",
    "for key, value in env.P[4].items():\n",
    "    print(key, ':', value)\n",
    "print('state 5')\n",
    "for key, value in env.P[5].items():\n",
    "    print(key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T05:18:56.108289Z",
     "start_time": "2020-04-04T05:18:56.103338Z"
    }
   },
   "outputs": [],
   "source": [
    "# define policy execute one episode similar to MC\n",
    "def execute_policy(env, policy, render=False):\n",
    "    total_steps,total_reward, steps = 0, 0, 0\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(0.3)\n",
    "        action = np.random.choice(env.nA, p=policy[observation])\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            if render:\n",
    "                print('total steps: %d' % steps)\n",
    "                print(\"return of this episode: \", total_reward)\n",
    "                time.sleep(3)\n",
    "                clear_output()\n",
    "            break\n",
    "    return total_steps, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T06:18:26.554293Z",
     "start_time": "2020-04-04T06:18:26.548309Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma=1.0, threshold=1e-10):\n",
    "    value_table = np.zeros(env.nS)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        # for each state\n",
    "        for state in range(env.nS):\n",
    "            Q = []\n",
    "            for action in range(env.nA): # full backup\n",
    "                # the dynamic of each (state, action) tuple\n",
    "                q = sum([trans_prob*(reward + gamma*value_table[next_state]*(1.0 - done)) \\\n",
    "                          for trans_prob, next_state, reward, done in env.P[state][action]])\n",
    "                Q.append(q)\n",
    "            vs = sum(policy[state] * Q)\n",
    "            delta = max(delta, abs(vs-value_table[state]))\n",
    "            value_table[state] = vs\n",
    "        if  delta < threshold:\n",
    "            break\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T06:19:59.442919Z",
     "start_time": "2020-04-04T06:19:59.436936Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# policy improvement\n",
    "def improve_policy(env, value_table, policy, gamma=1.0):    \n",
    "    optimal = True\n",
    "    execute_policy(env, policy, render=True)\n",
    "    # calculate q\n",
    "    for state in range(env.nS):\n",
    "        Q_table = np.zeros(env.nA)\n",
    "        for action in range(env.nA):\n",
    "            Q_table[action] = sum([trans_prob*(reward + gamma*value_table[next_state]*(1.0-done)) \\\n",
    "                           for trans_prob, next_state, reward, done in env.P[state][action]]) \n",
    "        a = np.argmax(Q_table)\n",
    "        if policy[state][a] != 1.:\n",
    "            optimal = False\n",
    "            policy[state] = 0.\n",
    "            policy[state][a] = 1.\n",
    "    return optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T08:30:41.382189Z",
     "start_time": "2020-04-04T08:30:41.376251Z"
    }
   },
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "def iterate_policy(env, gamma=1.0, threshold=1e-10):\n",
    "    policy = np.ones((env.nS, env.nA)) / env.nA\n",
    "    for i in range(1000):\n",
    "        print('{:=^80}'.format('iteration %i'%(i+1)))\n",
    "        time.sleep(3)\n",
    "        value_table = evaluate_policy(env, policy, gamma, threshold)\n",
    "        print(\"value_table: \", value_table)\n",
    "        time.sleep(2)\n",
    "        if improve_policy(env, value_table, policy, gamma):\n",
    "            print(\"iterated %d times\" %(i+1))\n",
    "            break\n",
    "    return policy, value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T08:30:42.551156Z",
     "start_time": "2020-04-04T08:30:42.546145Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the so called optimal policy's performance\n",
    "def check_policy(policy, episodes=100):\n",
    "    successed_nums = 0\n",
    "    total_steps = 0\n",
    "    for i in range(episodes):\n",
    "        one_episode_steps, one_episode_return = execute_policy(env, policy)\n",
    "        total_steps += one_episode_steps\n",
    "        if one_episode_return == 1.0:\n",
    "            successed_nums += 1\n",
    "    return total_steps / episodes, successed_nums / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T08:31:20.361471Z",
     "start_time": "2020-04-04T08:30:44.190218Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterated 3 times\n",
      "policy: \n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "value_tabel: \n",
      "[0.82352941 0.82352941 0.82352941 0.82352941 0.82352941 0.\n",
      " 0.52941176 0.         0.82352941 0.82352941 0.76470588 0.\n",
      " 0.         0.88235294 0.94117647 0.        ]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value_tabel = iterate_policy(env)\n",
    "print(\"policy: \", optimal_policy, sep='\\n')\n",
    "print(\"value_tabel: \", optimal_value_tabel, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-04T06:20:53.160885Z",
     "start_time": "2020-04-04T06:20:47.178877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave_steps:  44.2862\n",
      "acc:  0.7504\n"
     ]
    }
   ],
   "source": [
    "ave_steps, acc = check_policy(optimal_policy, episodes=5000)\n",
    "print(\"ave_steps: \", ave_steps)\n",
    "print(\"acc: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
