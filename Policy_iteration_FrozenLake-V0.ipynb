{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:53.976996Z",
     "start_time": "2020-04-03T14:33:53.860070Z"
    }
   },
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "import gym\n",
    "import numpy as np\n",
    "import time\n",
    "from IPython.display import clear_output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:53.988302Z",
     "start_time": "2020-04-03T14:33:53.977754Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "859\n"
     ]
    }
   ],
   "source": [
    "# show all environments\n",
    "total_env = envs.registry.all()\n",
    "env_ids = [i.id for i in total_env]\n",
    "print(len(env_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.280524Z",
     "start_time": "2020-04-03T14:33:53.990297Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "#show the FrozenLake-v0 environment\n",
    "try:\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "except:\n",
    "    print(\"unable to load env!\")\n",
    "env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.286509Z",
     "start_time": "2020-04-03T14:33:54.281521Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(4) <class 'gym.spaces.discrete.Discrete'>\n",
      "Discrete(16) <class 'gym.spaces.discrete.Discrete'>\n",
      "0\n",
      "True\n",
      "4\n",
      "4 16\n"
     ]
    }
   ],
   "source": [
    "# several key points of the environment\n",
    "action_space = env.action_space\n",
    "observation_space = env.observation_space\n",
    "print(action_space, type(action_space))\n",
    "print(observation_space, type(observation_space))\n",
    "print(action_space.sample()) # randomly sample from 0,1,2,3\n",
    "print(observation_space.contains(14)) # judge wheather the state is in the state set\n",
    "print(action_space.n) # number of actions\n",
    "print(env.nA, env.nS) # number of actions in this env, number of states in this env.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.294486Z",
     "start_time": "2020-04-03T14:33:54.287505Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state 15\n",
      "0 : [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False)]\n",
      "1 : [(0.3333333333333333, 13, 0.0, False), (0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True)]\n",
      "2 : [(0.3333333333333333, 14, 0.0, False), (0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False)]\n",
      "3 : [(0.3333333333333333, 15, 1.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 13, 0.0, False)]\n",
      "\n",
      "\n",
      "\n",
      "state 5\n",
      "0 : [(0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False)]\n",
      "1 : [(0.3333333333333333, 4, 0.0, False), (0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True)]\n",
      "2 : [(0.3333333333333333, 8, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False)]\n",
      "3 : [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 0, 0.0, False), (0.3333333333333333, 4, 0.0, False)]\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "check the 14th state's dynamic\n",
    "each state has four actions: 0=>up; 1=>down; 2=>left; 3=>right\n",
    "\n",
    "Notice that dynamic is different from policy.\n",
    "Policy determins the probability of choosing a action at a specific state.\n",
    "Dynamic relates to the environment, for example, imagine there's a boat driving on \n",
    "the lake, at a specific state it turned left, however, the place it finally reached\n",
    "is determined by the whole environment such as wind, stream, etc.\n",
    "\n",
    "When arrived at state[15], get 1 reward, then the episode is finished.\n",
    "When agent stuck into a \"Hole\" the episode finishs without any reward!\n",
    "'''\n",
    "print(\"state 15\")\n",
    "for key, value in env.P[14].items():\n",
    "    print(key,':', value)\n",
    "print('\\n'*2)\n",
    "print('state 5')\n",
    "for key, value in env.P[4].items():\n",
    "    print(key, ':', value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.302466Z",
     "start_time": "2020-04-03T14:33:54.295484Z"
    }
   },
   "outputs": [],
   "source": [
    "# define policy execute one episode similar to MC\n",
    "def execute_policy(env, policy, render=False):\n",
    "    total_steps,total_reward, steps = 0, 0, 0\n",
    "    observation = env.reset()\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(0.3)\n",
    "        action = np.random.choice(action_space.n, p=policy[observation])\n",
    "        observation, reward, done, _ = env.step(action)\n",
    "        steps += 1\n",
    "        total_steps += 1\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            if render:\n",
    "                print('total steps: %d' % steps)\n",
    "                print(\"return of this episode: \", total_reward)\n",
    "                time.sleep(3)\n",
    "                clear_output()\n",
    "            break\n",
    "    return total_steps, total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.311476Z",
     "start_time": "2020-04-03T14:33:54.303462Z"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate_policy(env, policy, gamma=1.0, threshold=1e-2):\n",
    "    value_table = np.zeros(observation_space.n)\n",
    "    while True:\n",
    "        delta = 0.0\n",
    "        # for each state\n",
    "        for state in range(env.nS):\n",
    "            action = np.random.choice(action_space.n, p=policy[state])\n",
    "            # the dynamic of each (state, action) tuple\n",
    "            q = sum([trans_prob*(reward + gamma*value_table[next_state]*(1.0 - done)) \\\n",
    "                      for trans_prob, next_state, reward, done in env.P[state][action]])\n",
    "            vs = sum(policy[state]*q)            \n",
    "            delta = max(delta, abs(vs-value_table[state]))\n",
    "            value_table[state] = vs\n",
    "        if  delta < threshold:\n",
    "            break\n",
    "    return value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.320418Z",
     "start_time": "2020-04-03T14:33:54.313438Z"
    }
   },
   "outputs": [],
   "source": [
    "# policy improvement\n",
    "def improve_policy(env, value_table, policy, gamma=1.0):    \n",
    "    optimal = True\n",
    "    execute_policy(env, policy, render=True)\n",
    "    # calculate q\n",
    "    for state in range(observation_space.n):\n",
    "        Q_table = np.zeros(env.nA)\n",
    "        for action in range(action_space.n):\n",
    "            Q_table[action] = sum([trans_prob*(reward + gamma*value_table[next_state]*(1.0-done)) \\\n",
    "                           for trans_prob, next_state, reward, done in env.P[state][action]]) \n",
    "        a = np.argmax(Q_table)\n",
    "        if policy[state][a] != 1.:\n",
    "            optimal = False\n",
    "            policy[state] = 0.\n",
    "            policy[state][a] = 1.\n",
    "    return optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.327399Z",
     "start_time": "2020-04-03T14:33:54.321414Z"
    }
   },
   "outputs": [],
   "source": [
    "# policy iteration\n",
    "def iterate_policy(env, gamma=1.0, threshold=1e-2):\n",
    "    policy = np.ones((env.nS, env.nA)) / env.nA\n",
    "    for i in range(1000):\n",
    "        print('{:=^80}'.format('iteration %i'%(i+1)))\n",
    "        time.sleep(3)\n",
    "        value_table = evaluate_policy(env, policy, gamma, threshold)\n",
    "        if improve_policy(env, value_table, policy, gamma):\n",
    "            print(\"iterated %d times\" %(i+1))\n",
    "            break\n",
    "    return policy, value_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:33:54.334421Z",
     "start_time": "2020-04-03T14:33:54.328396Z"
    }
   },
   "outputs": [],
   "source": [
    "# check the so called optimal policy's performance\n",
    "def check_policy(policy, episodes=100):\n",
    "    successed_nums = 0\n",
    "    total_steps = 0\n",
    "    for i in range(episodes):\n",
    "        one_episode_steps, one_episode_return = execute_policy(env, policy)\n",
    "        total_steps += one_episode_steps\n",
    "        if one_episode_return == 1.0:\n",
    "            successed_nums += 1\n",
    "    return total_steps / episodes, successed_nums / episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:34:30.924338Z",
     "start_time": "2020-04-03T14:33:54.336411Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterated 3 times\n",
      "policy: \n",
      "[[1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "value_tabel: \n",
      "[0.64119936 0.58473891 0.54572729 0.52592627 0.65977523 0.\n",
      " 0.40747796 0.         0.68879458 0.72588725 0.67960971 0.\n",
      " 0.         0.81450391 0.90665594 0.        ]\n"
     ]
    }
   ],
   "source": [
    "optimal_policy, optimal_value_tabel = iterate_policy(env)\n",
    "print(\"policy: \", optimal_policy, sep='\\n')\n",
    "print(\"value_tabel: \", optimal_value_tabel, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-04-03T14:34:36.989833Z",
     "start_time": "2020-04-03T14:34:30.925337Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ave_steps:  44.3846\n",
      "acc:  0.7452\n"
     ]
    }
   ],
   "source": [
    "ave_steps, acc = check_policy(optimal_policy, episodes=5000)\n",
    "print(\"ave_steps: \", ave_steps)\n",
    "print(\"acc: \", acc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
